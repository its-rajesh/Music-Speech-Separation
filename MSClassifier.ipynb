{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOfDtLFsB5Dg8MeCEDrTUFg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/its-rajesh/Music-Speech-Separation/blob/main/MSClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MUSIC SPEECH CLASSIFIER**"
      ],
      "metadata": {
        "id": "ioS14CbsHisS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6WcuVWOJHaxg"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import librosa as lb\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "cV_YcCIPH1oF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading STFT & Chroma STFT Features"
      ],
      "metadata": {
        "id": "oQyGXVx_H-A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SXPlgnSH82g",
        "outputId": "5713fca4-fe5f-4123-c00b-e40e9ae5eb6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/drive/My Drive/Projects/Music-Speech-Separation/Dataset/'"
      ],
      "metadata": {
        "id": "06Y2HUcbIDFj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folders = os.listdir(path)\n",
        "folders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zd_SHCAJi77",
        "outputId": "1a34379b-52bc-4d25-e7a4-11caa670b275"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['music', 'Mixture', 'Overlay', 'speech']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audiofiles = []\n",
        "for folder in folders:\n",
        "    files = os.listdir(path+'/'+folder)\n",
        "    files = sorted(files)\n",
        "    audio = []\n",
        "    for file in files:\n",
        "        data, samplerate = lb.load(path+'/'+folder+'/'+file, sr=8000, mono=True)\n",
        "        audio.append(data)\n",
        "    audiofiles.append(audio)"
      ],
      "metadata": {
        "id": "YAew3c-pJ3eG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "music = np.array(audiofiles[0])\n",
        "mixture = np.array(audiofiles[1])\n",
        "overlay = np.array(audiofiles[2])\n",
        "speech = np.array(audiofiles[3])\n",
        "\n",
        "dataset = [music, mixture, overlay, speech]"
      ],
      "metadata": {
        "id": "oh9nercUKhVM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "music[0], len(music[0]), len(music[0])/8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKLAfv4eYKYf",
        "outputId": "e9755ef6-be7d-43e6-bcfc-205ac3dc2949"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.06211764, -0.17527802, -0.13408872, ...,  0.06697536,\n",
              "         0.07972796,  0.11764227], dtype=float32), 120000, 15.0)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def createchunks(audio):\n",
        "  start = 0\n",
        "  stop = 4000 # for 0.5 seconds (8Khz): 1 sec has 8000 samples.\n",
        "  chunks = []\n",
        "  for i in range(len(audio)//4000):\n",
        "    chunks.append(audio[start:stop])\n",
        "    start = stop\n",
        "    stop = start+4000\n",
        "\n",
        "  return chunks\n",
        "\n",
        "#x = np.array(createchunks(music[0]))\n",
        "#print(x.shape)"
      ],
      "metadata": {
        "id": "Zk0QZwp8ZKYx"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' #SAMPLE FOR CHUNKS STFT\n",
        "chunks = np.array(createchunks(music[0]))\n",
        "spect = []\n",
        "for i in chunks:\n",
        "  spect.append(np.abs(lb.stft(i,n_fft=512))) #window length = nfft and hop length = win length //4\n",
        "\n",
        "spect = np.array(spect)\n",
        "spect.shape\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "8CfHfW_OZLbb",
        "outputId": "1f068733-5276-4545-8b1b-c0e4ce4c19c0"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' #SAMPLE FOR CHUNKS STFT\\nchunks = np.array(createchunks(music[0]))\\nspect = []\\nfor i in chunks:\\n  spect.append(np.abs(lb.stft(i,n_fft=512))) #window length = nfft and hop length = win length //4\\n\\nspect = np.array(spect)\\nspect.shape\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### STFT Features"
      ],
      "metadata": {
        "id": "XHND2AnpfR1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasetstft = []\n",
        "index = 0\n",
        "y, y_train = [], []\n",
        "for data in [music, speech]:\n",
        "    spect = []\n",
        "    for audio in data:\n",
        "      chunks = np.array(createchunks(audio))\n",
        "      chunks_spect = []\n",
        "      for i in chunks:\n",
        "        chunks_spect.append(np.abs(lb.stft(i,n_fft=512))) #window length = nfft and hop length = win length //4\n",
        "\n",
        "      chunks_spect = np.array(chunks_spect)\n",
        "\n",
        "      spect.append(chunks_spect)\n",
        "\n",
        "      if index == 0:\n",
        "        y.append(np.zeros(30))\n",
        "      else:\n",
        "        y.append(np.ones(30))\n",
        "    \n",
        "    index += 1\n",
        "    y_train.append(np.array(y))\n",
        "    datasetstft.append(np.array(spect))"
      ],
      "metadata": {
        "id": "PgvWvYCuZLXZ"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0].shape, len(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hQ4l6fXkJ0Y",
        "outputId": "bdb7bda1-9a10-4b96-c711-32425ed5dd80"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50, 30), 2)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetstft[0].shape, len(datasetstft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDVFcam2ZLRo",
        "outputId": "a782ab04-088c-47dd-8a41-249177955c5f"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50, 30, 257, 32), 2)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Chroma Features"
      ],
      "metadata": {
        "id": "80hvf3XNghC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasetchroma = []\n",
        "for data in [music, speech]:\n",
        "    chroma_spect = []\n",
        "    for audio in data:\n",
        "      chunks = np.array(createchunks(audio))\n",
        "      chunks_chroma = []\n",
        "      for i in chunks:\n",
        "        chunks_chroma.append(lb.feature.chroma_stft(y=audio)) \n",
        "\n",
        "      chunks_chroma = np.array(chunks_chroma)\n",
        "\n",
        "      chroma_spect.append(chunks_chroma)\n",
        "    datasetchroma.append(np.array(chroma_spect))"
      ],
      "metadata": {
        "id": "f5Tx-3gtZLE0"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tv2Qdo6IMSgl"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Reshaping the training datasets"
      ],
      "metadata": {
        "id": "gEBt4gTNnmFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xo5oWFuyL2Vd"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN MODEL"
      ],
      "metadata": {
        "id": "3qRbuUHeTOfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "xJYbOyUTTOXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input,Dense,Flatten,Dropout,Conv2D,MaxPool2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers"
      ],
      "metadata": {
        "id": "gy2SNW6BL4WF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 10\n",
        "inp = Input(shape =(257, 32, 1))\n",
        "out_dim = 2"
      ],
      "metadata": {
        "id": "T5aUxJRuTaHU"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "\n",
        "x = Conv2D(32, 3, activation='relu')(inp)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Dropout(0.30)(x)\n",
        "\n",
        "x = Conv2D(64, 3, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Dropout(0.30)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(100, activation='relu')(x)\n",
        "x = Dropout(0.30)(x)\n",
        "\n",
        "x = Dense(out_dim, activation='softmax')(x)"
      ],
      "metadata": {
        "id": "YQTtwAudTi5g"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(datasetstft, dtype=object)\n",
        "y_train = np.array(y_train, dtype=object)"
      ],
      "metadata": {
        "id": "0-2XtZBXT7wH"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify = Model(inputs = inp, outputs = x)\n",
        "classify.compile(loss='SparseCategoricalCrossentropy', optimizer = 'adam', metrics='accuracy') \n",
        "#classify.summary()\n",
        "classify_train = classify.fit(x_train,y_train, epochs = epochs ,batch_size=batch_size)\n",
        "end = time.time()\n",
        "print(\"time taken\",time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "Vvca_2TrTroW",
        "outputId": "7cb65fba-514b-4202-f13b-fe8091e05372"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-c7cd29f7f984>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SparseCategoricalCrossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#classify.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclassify_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time taken\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "boyx0OVdUXHD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}